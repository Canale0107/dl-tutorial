{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application oriented of LLMs\n",
    "\n",
    "outline\n",
    "- datasets\n",
    "    - for summarization\n",
    "    - for question answering\n",
    "\n",
    "- models\n",
    "    - instruct finetuning model\n",
    "\n",
    "- customizing prompts for the corresponding tasks\n",
    "    - summarization\n",
    "    - question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# datasets\n",
    "\n",
    "summarization dataset: DIALOGSum (https://huggingface.co/datasets/knkarthick/dialogsum), License: cc-by-nc-sa-4.0<br>\n",
    "question and answering dataset: SQuAD2.0 (https://huggingface.co/datasets/squad_v2), License:cc-by-sa-4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load summarization dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/tslab/phusaeng/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd36827d3490488d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "687dc02de0b44e39a86bb2f9d5a8dc56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summarization_dataset = load_dataset(\"knkarthick/dialogsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarization_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load question-answering dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset squad_v2 (/home/tslab/phusaeng/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "279e132a189f4665ab654f3dfbda9501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 130319\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 11873\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_dataset = load_dataset(\"squad_v2\")\n",
    "qa_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load model\n",
    "\n",
    "Instruction finetuning models with Flan-T5 models (https://arxiv.org/pdf/2210.11416.pdf):\n",
    "- flan t5 base (https://huggingface.co/google/flan-t5-base): Apache License 2.0\n",
    "- flan t5 large (https://huggingface.co/google/flan-t5-large): Apache License 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base model\n",
    "tokenizer_base = AutoTokenizer.from_pretrained('google/flan-t5-base')\n",
    "t5_base = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base', torch_dtype=torch.bfloat16) # bfloat16 is faster than fp32\n",
    "t5_base = t5_base.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# large model\n",
    "tokenizer_large = AutoTokenizer.from_pretrained('google/flan-t5-large')\n",
    "t5_large = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-large', torch_dtype=torch.bfloat16) # bfloat16 is faster than fp32\n",
    "t5_large = t5_large.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base model parameters: trainable_params: 247,577,856\n",
      "all_params: 247,577,856\n",
      "percentage of trainable params: 100.0%\n",
      "large model parameters: trainable_params: 783,150,080\n",
      "all_params: 783,150,080\n",
      "percentage of trainable params: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# count params\n",
    "def count_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    def num_to_str(num):\n",
    "        return format(num, ',')\n",
    "    return f\"trainable_params: {num_to_str(trainable_params)}\\nall_params: {num_to_str(all_params)}\\npercentage of trainable params: {100*trainable_params/all_params}%\"\n",
    "print(f\"base model parameters: {count_parameters(t5_base)}\")\n",
    "print(f\"large model parameters: {count_parameters(t5_large)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# customizing prompts for the corresponding tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## summarization task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=1000):\n",
    "  # Tokenize\n",
    "  input_ids = tokenizer.encode(\n",
    "          text,\n",
    "          return_tensors=\"pt\",\n",
    "          truncation=True,\n",
    "          max_length=max_input_tokens\n",
    "  )\n",
    "  # Generate\n",
    "  device = model.device\n",
    "  # print(f'device of the model: {device}')\n",
    "  generated_tokens_with_prompt = model.generate(\n",
    "    input_ids=input_ids.to(device),\n",
    "    max_length=max_output_tokens,\n",
    "    # do_sample=True,\n",
    "    # top_k=1,\n",
    "  )\n",
    "  # Decode\n",
    "  generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
    "  generated_text_answer = generated_text_with_prompt[0]\n",
    "  return generated_text_answer\n",
    "\n",
    "def summarize(\n",
    "    dialogue,\n",
    "    model, \n",
    "    tokenizer,\n",
    "    prompt_format=None,\n",
    "):\n",
    "  if prompt_format is None:\n",
    "    prompt = \"\"\"\n",
    "    Summarize the following conversation.\n",
    "\n",
    "    {dialogue}\n",
    "\n",
    "    Summary:\n",
    "    \"\"\"\n",
    "  else:\n",
    "    prompt = prompt_format\n",
    "  # print(f'INPUT PROMPT:\\n{prompt.format(dialogue=dialogue)}')\n",
    "  output = inference(prompt.format(dialogue=dialogue), model, tokenizer)\n",
    "  return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### default prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT:\n",
      "#Person1#: Hello Mrs. Parker, how have you been?\n",
      "#Person2#: Hello Dr. Peters. Just fine thank you. Ricky and I are here for his vaccines.\n",
      "#Person1#: Very well. Let's see, according to his vaccination record, Ricky has received his Polio, Tetanus and Hepatitis B shots. He is 14 months old, so he is due for Hepatitis A, Chickenpox and Measles shots.\n",
      "#Person2#: What about Rubella and Mumps?\n",
      "#Person1#: Well, I can only give him these for now, and after a couple of weeks I can administer the rest.\n",
      "#Person2#: OK, great. Doctor, I think I also may need a Tetanus booster. Last time I got it was maybe fifteen years ago!\n",
      "#Person1#: We will check our records and I'll have the nurse administer and the booster as well. Now, please hold Ricky's arm tight, this may sting a little.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "BASELINE OUTPUT:\n",
      "Mrs Parker takes Ricky for his vaccines. Dr. Peters checks the record and then gives Ricky a vaccine.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "BASE MODEL OUTPUT:\n",
      "Ricky is coming to the office today.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "LARGE MODEL OUTPUT:\n",
      "Ricky is coming to the doctor for his vaccinations.\n"
     ]
    }
   ],
   "source": [
    "idx = 1\n",
    "dialogue_test = summarization_dataset['train'][:100]['dialogue']\n",
    "baseline_output = summarization_dataset['train'][:100]['summary']\n",
    "output_base = summarize(\n",
    "    dialogue_test[idx],\n",
    "    t5_base,\n",
    "    tokenizer_base,\n",
    "    prompt_format=None,\n",
    ")\n",
    "output_large = summarize(\n",
    "    dialogue_test[idx],\n",
    "    t5_large,\n",
    "    tokenizer_large,\n",
    "    prompt_format=None,\n",
    ")\n",
    "print(f\"INPUT:\\n{dialogue_test[idx]}\")\n",
    "print('-'*100)\n",
    "print(f'BASELINE OUTPUT:\\n{baseline_output[idx]}')\n",
    "print('-'*100)\n",
    "print(f'BASE MODEL OUTPUT:\\n{output_base}')\n",
    "print('-'*100)\n",
    "print(f'LARGE MODEL OUTPUT:\\n{output_large}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### customize prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT:\n",
      "#Person1#: Hello Mrs. Parker, how have you been?\n",
      "#Person2#: Hello Dr. Peters. Just fine thank you. Ricky and I are here for his vaccines.\n",
      "#Person1#: Very well. Let's see, according to his vaccination record, Ricky has received his Polio, Tetanus and Hepatitis B shots. He is 14 months old, so he is due for Hepatitis A, Chickenpox and Measles shots.\n",
      "#Person2#: What about Rubella and Mumps?\n",
      "#Person1#: Well, I can only give him these for now, and after a couple of weeks I can administer the rest.\n",
      "#Person2#: OK, great. Doctor, I think I also may need a Tetanus booster. Last time I got it was maybe fifteen years ago!\n",
      "#Person1#: We will check our records and I'll have the nurse administer and the booster as well. Now, please hold Ricky's arm tight, this may sting a little.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "BASELINE OUTPUT:\n",
      "Mrs Parker takes Ricky for his vaccines. Dr. Peters checks the record and then gives Ricky a vaccine.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "BASE MODEL OUTPUT:\n",
      "#Person1#: Dr. Peters.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "LARGE MODEL OUTPUT:\n",
      "#Person2 is Mrs. Parker. She is here for her 14 month old son Ricky's vaccinations.\n"
     ]
    }
   ],
   "source": [
    "new_prompt_format = \"\"\"\n",
    "### Input:\n",
    "Summarize the following conversation step-by-step:\n",
    "{dialogue}\n",
    "\n",
    "### Output:\n",
    "What is the name of person2?\n",
    "\"\"\"\n",
    "# What is #person2# trying to do in the conversation\n",
    "output_base = summarize(\n",
    "    dialogue_test[idx],\n",
    "    t5_base,\n",
    "    tokenizer_base,\n",
    "    prompt_format=new_prompt_format,\n",
    ")\n",
    "output_large = summarize(\n",
    "    dialogue_test[idx],\n",
    "    t5_large,\n",
    "    tokenizer_large,\n",
    "    prompt_format=new_prompt_format,\n",
    ")\n",
    "print(f\"INPUT:\\n{dialogue_test[idx]}\")\n",
    "print('-'*100)\n",
    "print(f'BASELINE OUTPUT:\\n{baseline_output[idx]}')\n",
    "print('-'*100)\n",
    "print(f'BASE MODEL OUTPUT:\\n{output_base}')\n",
    "print('-'*100)\n",
    "print(f'LARGE MODEL OUTPUT:\\n{output_large}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## question-answering tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### default prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT:\n",
      "When did Beyonce start becoming popular?\n",
      "----------------------------------------------------------------------------------------------------\n",
      "BASELINE ANSWER:\n",
      "in the late 1990s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "BASE MODEL OUTPUT:\n",
      "in the early 1980s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "LARGE MODEL OUTPUT:\n",
      "1990\n"
     ]
    }
   ],
   "source": [
    "idx = 0 \n",
    "test_qa = qa_dataset['train']['question'][idx]\n",
    "answer_qa = qa_dataset['train']['answers'][idx]['text'][0]\n",
    "output_base = inference(test_qa, t5_base, tokenizer_base)\n",
    "output_large = inference(test_qa, t5_large, tokenizer_large)\n",
    "print(f\"INPUT:\\n{test_qa}\")\n",
    "print('-'*100)\n",
    "print(f'BASELINE ANSWER:\\n{answer_qa}')\n",
    "print('-'*100)\n",
    "print(f\"BASE MODEL OUTPUT:\\n{output_base}\")\n",
    "print('-'*100)\n",
    "print(f\"LARGE MODEL OUTPUT:\\n{output_large}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### customize prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT:\n",
      "When did Beyonce start becoming popular? explain in step-by-step\n",
      "----------------------------------------------------------------------------------------------------\n",
      "BASELINE ANSWER:\n",
      "in the late 1990s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "BASE MODEL OUTPUT:\n",
      "Beyonce started becoming popular in the early 1980s.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "LARGE MODEL OUTPUT:\n",
      "Beyonce became popular in the late 1990s.\n"
     ]
    }
   ],
   "source": [
    "test_qa = qa_dataset['train']['question'][idx] + \" explain in step-by-step\" # <-- add more prompt\n",
    "answer_qa = qa_dataset['train']['answers'][idx]['text'][0]\n",
    "output_base = inference(test_qa, t5_base, tokenizer_base)\n",
    "output_large = inference(test_qa, t5_large, tokenizer_large)\n",
    "print(f\"INPUT:\\n{test_qa}\")\n",
    "print('-'*100)\n",
    "print(f'BASELINE ANSWER:\\n{answer_qa}')\n",
    "print('-'*100)\n",
    "print(f\"BASE MODEL OUTPUT:\\n{output_base}\")\n",
    "print('-'*100)\n",
    "print(f\"LARGE MODEL OUTPUT:\\n{output_large}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "pytorch2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
