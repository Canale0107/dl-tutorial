{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_imagerecog_intro.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuYr1xEpJcg6"
      },
      "source": [
        "2021 Kosuke Mori and Takahiro Shinozaki @ Tokyo Tech\n",
        "\n",
        "Quick introduction of image recognition based on neural network with MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bhVubdRIjLp"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtE0vs2FoFaa"
      },
      "source": [
        "# Check if GPU is available"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7zsv23KJvy3"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    print('CUDA（GPU）is available')\n",
        "    device = 'cuda'\n",
        "else:\n",
        "    print('CUDA（GPU）is not available')\n",
        "    device = 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4vnGrSJmF9d"
      },
      "source": [
        "# Load the MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ah8aEcGSJx9x"
      },
      "source": [
        "dataset_traindev = torchvision.datasets.MNIST(\n",
        "    root='./data', train=True, download=True, transform=transforms.ToTensor()\n",
        ")\n",
        "dataset_test = torchvision.datasets.MNIST(\n",
        "    root='./data', train=False, download=True, transform=transforms.ToTensor()\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozkfVeZ4mQz7"
      },
      "source": [
        "# Display data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhKWHh0mmc1-"
      },
      "source": [
        "img, label = dataset_traindev[0]\n",
        "c, h, w = img.size()\n",
        "print(f'Image data info: C={c}, H={h}, W={w} ({img.size()})')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xi9yZC0ymRkf"
      },
      "source": [
        "fig = plt.figure(figsize=(10, 5))\n",
        "for i in range(10):\n",
        "    for j in range(len(dataset_traindev)):\n",
        "        img, label = dataset_traindev[j]\n",
        "        if label == i:\n",
        "            break\n",
        "    ax = fig.add_subplot(2, 5, i+1)\n",
        "    ax.axis('off')\n",
        "    ax.set_title('Number:' + str(label))\n",
        "    ax.imshow(img.squeeze(0), cmap='gray')\n",
        "fig.subplots_adjust(wspace=0.1, hspace=0.1)\n",
        "fig.suptitle('Image examples in MNIST', fontsize=18)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoeC7792qXqh"
      },
      "source": [
        "# Prepare training, development, and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksAYRQ_tRTJD"
      },
      "source": [
        "TrainRate = 0.8\n",
        "BatchSize = 512\n",
        "\n",
        "# Split the dataset for training into the training and development sets\n",
        "num_train = int(len(dataset_traindev) * TrainRate)\n",
        "num_val = len(dataset_traindev) - num_train\n",
        "dataset_train, dataset_dev = random_split(dataset_traindev, [num_train, num_val])\n",
        "\n",
        "# Prepare data loader for mini-batch training\n",
        "loader_train = DataLoader(dataset_train, batch_size=BatchSize, shuffle=True, drop_last=True)\n",
        "loader_dev = DataLoader(dataset_dev, batch_size=BatchSize)\n",
        "loader_test = DataLoader(dataset_test, batch_size=BatchSize)\n",
        "\n",
        "print('# train samples:', len(dataset_train))\n",
        "print('# development samples:', len(dataset_dev))\n",
        "print('# test samples:', len(dataset_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmMGWGZtrgtt"
      },
      "source": [
        "# Define a neural network model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZPg3jSTTWRS"
      },
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.layer1 = nn.Linear(28 * 28, 100)\n",
        "        self.layer2 = nn.Linear(100, 20)\n",
        "        self.layer_out = nn.Linear(20, 10)\n",
        "\n",
        "    def forward(self, z):\n",
        "        z = nn.Sigmoid()(self.layer1(z))\n",
        "        z = nn.Sigmoid()(self.layer2(z))\n",
        "        z = self.layer_out(z)  # Pre-softmax output for nn.CrossEntropyLoss\n",
        "        return z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJkvFudOVfY1"
      },
      "source": [
        "# Make an instance of the neural network\n",
        "model = NeuralNetwork().to(device)\n",
        "params = model.state_dict()\n",
        "print('params =', params)\n",
        "print(params['layer1.weight'].device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQCXfldUr5Ri"
      },
      "source": [
        "# Prepare an optimizer and a loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJCl5u0bVhWO"
      },
      "source": [
        "# optimizer = optim.SGD(model.parameters(), lr=0.02)\n",
        "optimizer = optim.Adam(model.parameters(), lr=5e-3)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xW9xuHQXsOem"
      },
      "source": [
        "# Define a step of batch processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diAweOveVt-X"
      },
      "source": [
        "def train_step(x, y):\n",
        "    model.train() # set train mode\n",
        "\n",
        "    out = model(x) # forward propagation\n",
        "\n",
        "    optimizer.zero_grad() # clear gradients\n",
        "    loss = criterion(out, y)\n",
        "    loss.backward() # calculate gradient\n",
        "\n",
        "    optimizer.step() # update network parameters\n",
        "\n",
        "    with torch.no_grad(): # no gradient computation\n",
        "        # NOTE: If we omit this softmax operation, the inference results will be the same\n",
        "        out = nn.Softmax(1)(out) # apply softmax function for the model outputs\n",
        "        num_crr = (y == torch.argmax(out, 1)).sum()\n",
        "\n",
        "    return (loss.item(), num_crr.item()) # touple of loss and correct count\n",
        "\n",
        "def test_step(x, y):\n",
        "    model.eval() # set evaluation mode\n",
        "\n",
        "    out = model(x)\n",
        "\n",
        "    loss = criterion(out, y)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # NOTE: If we omit this softmax operation, the inference results will be the same\n",
        "        out = nn.Softmax(1)(out) # apply softmax function for the model outputs\n",
        "        num_crr = (y == torch.argmax(out, 1)).sum()\n",
        "\n",
        "    return (loss.item(), num_crr.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_ahSMEqsVyl"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plezgcjtVvTt",
        "collapsed": true
      },
      "source": [
        "NumEpocs = 20\n",
        "log = {'train_loss': [], 'train_acc': [], 'dev_loss': [], 'dev_acc': []}\n",
        "for epoch in range(NumEpocs):\n",
        "    train_loss_total = 0.0\n",
        "    train_num_crr_total = 0.0\n",
        "    dev_loss_total = 0.0\n",
        "    dev_num_crr_total = 0.0\n",
        "    num_train_sample = 0\n",
        "    num_dev_sample = 0\n",
        "\n",
        "    # mini-batch processings\n",
        "    for x, y in loader_train:\n",
        "        b, c, h, w = x.size()\n",
        "        x, y = x.view(b, c * h * w).to(device), y.to(device)\n",
        "        loss, num_crr = train_step(x, y)\n",
        "        train_loss_total += b * loss\n",
        "        train_num_crr_total += num_crr\n",
        "        num_train_sample += len(y)\n",
        "\n",
        "    for x, y in loader_dev:\n",
        "        b, c, h, w = x.size()\n",
        "        x, y = x.view(b, c * h * w).to(device), y.to(device)\n",
        "        loss, num_crr = test_step(x, y)\n",
        "        dev_loss_total += b * loss\n",
        "        dev_num_crr_total += num_crr\n",
        "        num_dev_sample += len(y)\n",
        "\n",
        "    train_loss_avg = train_loss_total / num_train_sample\n",
        "    train_acc_avg = train_num_crr_total / num_train_sample\n",
        "    dev_loss_avg = dev_loss_total / num_dev_sample\n",
        "    dev_acc_avg = dev_num_crr_total / num_dev_sample\n",
        "\n",
        "    log['train_loss'].append(train_loss_avg)\n",
        "    log['dev_loss'].append(dev_loss_avg)\n",
        "    log['train_acc'].append(train_acc_avg)\n",
        "    log['dev_acc'].append(dev_acc_avg)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{NumEpocs},' \\\n",
        "          f' train_loss: {train_loss_avg:.5f}, train_acc: {train_acc_avg:.5f},' \\\n",
        "          f' dev_loss: {dev_loss_avg:.5f}, dev_acc: {dev_acc_avg:.5f}')\n",
        "\n",
        "print('Done training')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpMt-v5ntuZD"
      },
      "source": [
        "# Display the learning curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xn6Ss5ummxas"
      },
      "source": [
        "fig = plt.figure(figsize=(10, 4))\n",
        "epochs = range(1, NumEpocs+1)\n",
        "# Draw the loss curve\n",
        "ax1 = fig.add_subplot(1, 2, 1)\n",
        "ax1.plot(epochs, log['train_loss'], label='Training')\n",
        "ax1.plot(epochs, log['dev_loss'], label='Development')\n",
        "ax1.set_title('Loss curve', fontsize=16)\n",
        "ax1.set_xlabel('Epochs', fontsize=14)\n",
        "ax1.set_ylabel('Loss', fontsize=14)\n",
        "ax1.set_xlim(1, NumEpocs)\n",
        "ax1.set_ylim(0,)\n",
        "ax1.grid(linestyle='--')\n",
        "ax1.legend(loc='upper right')\n",
        "# Draw the accuracy curve\n",
        "ax2 = fig.add_subplot(1, 2, 2)\n",
        "ax2.plot(epochs, log['train_acc'], label='Training')\n",
        "ax2.plot(epochs, log['dev_acc'], label='Development')\n",
        "ax2.set_title('Accuracy curve', fontsize=16)\n",
        "ax2.set_xlabel('Epochs', fontsize=14)\n",
        "ax2.set_ylabel('Accuracy', fontsize=14)\n",
        "ax2.set_xlim(1, NumEpocs)\n",
        "ax2.grid(linestyle='--')\n",
        "ax2.legend(loc='lower right')\n",
        "fig.subplots_adjust(wspace=0.3)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWiCw0GItw8n"
      },
      "source": [
        "# Evaluate the trained model on the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSgbfhe8Vyxh"
      },
      "source": [
        "model.eval() # set evaluation mode\n",
        "test_num_crr_total = 0.0\n",
        "num_test_sample = 0\n",
        "for x, y in loader_test:\n",
        "    b, c, h, w = x.size()\n",
        "    x, y = x.view(b, c * h * w).to(device), y.to(device)\n",
        "    _, num_crr = test_step(x, y)\n",
        "    test_num_crr_total += num_crr\n",
        "    num_test_sample += len(y)\n",
        "test_acc = test_num_crr_total / num_test_sample\n",
        "print('test_acc =', test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}