{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch_llm_intro_ja.ipynb\n",
    "\n",
    "こちらは日本語向けのLLM(Large Language Model)のチュートリアルです。<br>\n",
    "ここでは、[rinna株式会社](https://corp.rinna.co.jp/)によって学習された、日本語のGPTモデルを使います。<br>\n",
    "これらのモデルは、[The MIT license](https://opensource.org/license/mit/)で提供されています。\n",
    "\n",
    "このチュートリアルは、Google Colaboratoryで、GPUを使って実行されることを想定しています。\n",
    "\n",
    "This is the tutorial for LLM(Large Language Model) for Japanese.<br>\n",
    "We use Japanese GPT model trained by [rinna Co., Ltd.](https://corp.rinna.co.jp/).<br>\n",
    "These models are under [The MIT license](https://opensource.org/license/mit/).\n",
    "\n",
    "I assume that this tutorial is used on Google Colaboratory with GPU.\n",
    "\n",
    "-----\n",
    "\n",
    "https://huggingface.co/rinna/japanese-gpt-neox-small <br>\n",
    "https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-ppo\n",
    "\n",
    "これらが、今回使用したモデルのドキュメントとなっております。\n",
    "\n",
    "These are the pages of documents for the models that we use this time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ライブラリをpip installする\n",
    "\n",
    "ここでは、必要なライブラリをpipを用いてインストールします。\n",
    "\n",
    "On this part we will pip install library that we need.\n",
    "\n",
    "-----\n",
    "\n",
    "https://www.cloudnotes.tech/entry/rinna_gpt_jp\n",
    "\n",
    "こちらのページによると、もしうまく動かなかったらカーネルをリスタートする必要があるとのことです。\n",
    "\n",
    "According to this page, we have to restart kernel if SentencePiece doesn't work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers\n",
    "! pip install sentencepiece\n",
    "! pip install accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ライブラリをインポートする\n",
    "\n",
    "ここでは、必要なライブラリをインポートします。\n",
    "- Pytorch\n",
    "- transformers\n",
    "\n",
    "PytorchはGoogle Colaboratoryにはデフォルトでインストールされています。<br>\n",
    "[transformers](https://github.com/huggingface/transformers)は、[HuggingFace社](https://huggingface.co/)が開発を行っている機械学習向けのライブラリです。\n",
    "\n",
    "もし現時点でGPUが使えない状態の場合は、`False`が出力されます。<br>\n",
    "もし`False`が出力されたら、GPUを使えるよう変更してください。\n",
    "\n",
    "\n",
    "Here we import libraries that we need.\n",
    "- Pytorch\n",
    "- transformers\n",
    "\n",
    "Pytorch is pre-installed on Google Colaboratory.<br>\n",
    "[transformers](https://github.com/huggingface/transformers) is library for machine learning that [HuggingFace, Inc.](https://huggingface.co/) develops.\n",
    "\n",
    "If you can't use GPU now, this notebook outputs `False`.<br>\n",
    "If model outputs `False`, you should change setting to use GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 話の続きを予測する\n",
    "\n",
    "まずは、話の続きを予測するモデルを試してみましょう。\n",
    "\n",
    "For first, we use model that predicts continuation of the story."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TokenizerとModelを定義する\n",
    "\n",
    "ファイルを読み込んで、`tokenizer`と`model`を定義します。\n",
    "\n",
    "Read files and define `tokenizer` and `model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"rinna/japanese-gpt-neox-small\", use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"rinna/japanese-gpt-neox-small\")\n",
    "\n",
    "# もしcuda(GPU)が使用可能であれば、モデルをGPUに移します。\n",
    "# If cuda(GPU) is available, we throw model to GPU.\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Textを定義する\n",
    "\n",
    "`text`を定義します。<br>\n",
    "ここで`text`とは、続きを予測させる文章の出だしを表します。<br>\n",
    "**ここでは内容を自由に変更することができます。**\n",
    "\n",
    "Define `text`.<br>\n",
    "`text`, that we use for now, means the sentences whose continuation that we want.<br>\n",
    "**Here you can change codes freely.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"東京工業大学とは、\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 返答を予測する\n",
    "\n",
    "ここでは、モデルが続きを予測します。\n",
    "少し時間が掛かります。\n",
    "\n",
    "Here model predicts continuation.\n",
    "This part takes some times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = tokenizer.encode(text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        max_new_tokens=100,\n",
    "        min_new_tokens=100,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        top_k=50,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 会話の続きを予測する\n",
    "\n",
    "次に、会話の続きを予測するモデルを試してみましょう。\n",
    "\n",
    "Next, we use the model that predicts continuation of dialogue.\n",
    "\n",
    "### TokenizerとModelを定義する\n",
    "\n",
    "ファイルを読み込んで、`tokenizer`と`model`を定義します。<br>\n",
    "**モデルが大きいので、実行される際はストレージやRAMにご注意ください。**\n",
    "\n",
    "Read files and define `tokenizer` and `model`.<br>\n",
    "**Please pay attention to free space of your storage or RAM because the model is too big if you run this section.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2 = AutoTokenizer.from_pretrained(\"rinna/japanese-gpt-neox-3.6b-instruction-ppo\", use_fast=False)\n",
    "model2 = AutoModelForCausalLM.from_pretrained(\"rinna/japanese-gpt-neox-3.6b-instruction-ppo\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Promptを定義する\n",
    "\n",
    "`prompt`を定義します。<br>\n",
    "ここで`prompt`とは、言語モデルに渡す会話のことを表します。<br>\n",
    "**ここでは内容を自由に変更することができます。**\n",
    "\n",
    "Define `prompt`.<br>\n",
    "`prompt`, that we use for now, means conversation that we pass to language model.<br>\n",
    "**Here You can change codes freely.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\n",
    "    {\n",
    "        \"speaker\": \"ユーザー\",\n",
    "        \"text\": \"昔々あるところに、\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Promptを処理する\n",
    "\n",
    "先ほどのパートで定義した`prompt`を処理します。\n",
    "\n",
    "Here `prompt` that you defined last part is processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\n",
    "    f\"{uttr['speaker']}: {uttr['text']}\"\n",
    "    for uttr in prompt\n",
    "]\n",
    "prompt = \"<NL>\".join(prompt)\n",
    "prompt = (\n",
    "    prompt\n",
    "    + \"<NL>\"\n",
    "    + \"システム: \"\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 返答を予測する\n",
    "\n",
    "ここでは、モデルが返答を予測します。\n",
    "\n",
    "Here model predicts reply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids2 = tokenizer2.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids2 = model2.generate(\n",
    "        token_ids2.to(model2.device),\n",
    "        do_sample=True,\n",
    "        max_new_tokens=128,\n",
    "        temperature=0.7,\n",
    "        repetition_penalty=1.1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "output2 = tokenizer2.decode(output_ids2.tolist()[0][token_ids2.size(1):])\n",
    "output2 = output2.replace(\"<NL>\", \"\\n\")\n",
    "print(output2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_tutorial_ja_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
