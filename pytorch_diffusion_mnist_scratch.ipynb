{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Introduction to diffusion models\n",
    "\n",
    "In this notebook, we will create our own diffusion model from scratch. We will implement the simplify version of the forward process and the reverse process, where we are trying to directly predict the denoised image instead of predicting noise. We will be following the tutorial from the [Hugging Face's Diffusion Models from Scratch](https://colab.research.google.com/github/huggingface/diffusion-models-class/blob/main/unit1/02_diffusion_models_from_scratch.ipynb#scrollTo=DPTzwfAgYWGi).\n",
    "\n",
    "The main difference from the huggingface tutorials is that, in part one, we provide two variants of the UNet model, basic UNet and UNet with attention, ready to implement in the \"toy\" diffusion model we are going to build for generating MNIST data. In part two, our tutorial provide class-conditioned diffusion model for the basic UNet, which is not covered in the [Making a Class-Conditioned Diffusion Model](https://colab.research.google.com/github/huggingface/diffusion-models-class/blob/main/unit2/02_class_conditioned_diffusion_model_example.ipynb#scrollTo=k_d6S0Fwlahy).\n",
    "\n",
    "license: Apache-2.0 license\n",
    "\n",
    "references: \n",
    "- https://colab.research.google.com/github/huggingface/diffusion-models-class/blob/main/unit1/02_diffusion_models_from_scratch.ipynb#scrollTo=DPTzwfAgYWGi\n",
    "- https://colab.research.google.com/github/huggingface/diffusion-models-class/blob/main/unit2/02_class_conditioned_diffusion_model_example.ipynb#scrollTo=k_d6S0Fwlahy\n",
    "- https://github.com/huggingface/diffusion-models-class/tree/main?tab=Apache-2.0-1-ov-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installation\n",
    "!pip install diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.MNIST(root=\"data/mnist/\", train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "def trainDataPct(dataset, pct):\n",
    "    n = len(dataset)\n",
    "    n_train = int(n * pct)\n",
    "    n_val = n - n_train\n",
    "    train_dataset, val_dataset = random_split(dataset, [n_train, n_val]) # we don't need a validation set\n",
    "    return train_dataset\n",
    "\n",
    "training_percentage = 1.0\n",
    "print(f\"Choosing {training_percentage*100}% of the data for training.\")\n",
    "\n",
    "dataset = trainDataPct(dataset, pct=training_percentage)\n",
    "print(f\"Number of training samples: {len(dataset)}\")\n",
    "\n",
    "train_dataloader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_dataloader))\n",
    "print('Input shape:', x.shape)\n",
    "print('Labels:', y)\n",
    "plt.imshow(torchvision.utils.make_grid(x)[0], cmap='Greys');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 build our own duffision models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet2DModel\n",
    "\n",
    "class BasicUNet(nn.Module):\n",
    "    \"\"\"A minimal UNet implementation.\"\"\"\n",
    "    def __init__(self, in_channels=1, out_channels=1):\n",
    "        super().__init__()\n",
    "        self.down_layers = torch.nn.ModuleList([\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=5, padding=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=5, padding=2),\n",
    "            nn.Conv2d(64, 64, kernel_size=5, padding=2),\n",
    "        ])\n",
    "        self.up_layers = torch.nn.ModuleList([\n",
    "            nn.Conv2d(64, 64, kernel_size=5, padding=2),\n",
    "            nn.Conv2d(64, 32, kernel_size=5, padding=2),\n",
    "            nn.Conv2d(32, out_channels, kernel_size=5, padding=2),\n",
    "        ])\n",
    "        self.act = nn.SiLU() # The activation function\n",
    "        self.downscale = nn.MaxPool2d(2)\n",
    "        self.upscale = nn.Upsample(scale_factor=2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = []\n",
    "        for i, l in enumerate(self.down_layers):\n",
    "            x = self.act(l(x)) # Through the layer and the activation function\n",
    "            if i < 2: # For all but the third (final) down layer:\n",
    "                h.append(x) # Storing output for skip connection\n",
    "                x = self.downscale(x) # Downscale ready for the next layer\n",
    "        \n",
    "        for i, l in enumerate(self.up_layers):\n",
    "            if i > 0: # For all except the first up layer\n",
    "                x = self.upscale(x) # Upscale\n",
    "                x += h.pop() # Fetching stored output (skip connection)\n",
    "            x = self.act(l(x)) # Through the layer and the activation function\n",
    "        \n",
    "        return x\n",
    "\n",
    "class UNetWrapper(nn.Module):\n",
    "    \"\"\"A wrapper for the UNet model.\n",
    "\n",
    "    Picking whther to use the basic or the diffuser model.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"unet_mini\"):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        if model_name == \"unet_mini\":\n",
    "            self.model = BasicUNet()\n",
    "        elif model_name == \"unet_attn\":\n",
    "            self.model = UNet2DModel(\n",
    "                sample_size=28,  # the target image resolution\n",
    "                in_channels=1,  # the number of input channels, 3 for RGB images\n",
    "                out_channels=1,  # the number of output channels\n",
    "                layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
    "                block_out_channels=(32, 64, 64),  # Roughly matching our basic unet example\n",
    "                down_block_types=( \n",
    "                    \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "                    \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "                    \"AttnDownBlock2D\",\n",
    "                ), \n",
    "                up_block_types=(\n",
    "                    \"AttnUpBlock2D\", \n",
    "                    \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
    "                    \"UpBlock2D\",   # a regular ResNet upsampling block\n",
    "                ),\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Unknown model name\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.model_name == \"unet_mini\":\n",
    "            return self.model(x)\n",
    "        elif self.model_name == \"unet_attn\":\n",
    "            # use 0 for a kind of bypassing timestep embedding\n",
    "            return self.model(x, 0).sample\n",
    "\n",
    "    def count_params(self):\n",
    "        return sum(p.numel() for p in self.model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNetWrapper(\"unet_mini\")\n",
    "print(f\"Number of parameters of Unet Mini: {model.count_params()}\")\n",
    "model = UNetWrapper(\"unet_attn\")\n",
    "print(f\"Number of parameters of Unet Attn: {model.count_params()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Define diffusion process function\n",
    "Here we define a simple noising function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt(x, amount, rand_fn=\"rand\"):\n",
    "  \"\"\"Corrupt the input `x` by mixing it with noise according to `amount`\"\"\"\n",
    "  if rand_fn == \"rand\":\n",
    "    noise = torch.rand_like(x)\n",
    "  elif rand_fn == \"randn\":\n",
    "    noise = torch.randn_like(x)\n",
    "  amount = amount.view(-1, 1, 1, 1) # Sort shape so broadcasting works\n",
    "  return x*(1-amount) + noise*amount "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the input data\n",
    "fig, axs = plt.subplots(3, 1, figsize=(8, 8))\n",
    "axs[0].set_title('Input data')\n",
    "axs[0].imshow(torchvision.utils.make_grid(x)[0], cmap='Greys')\n",
    "\n",
    "# Adding noise\n",
    "amount = torch.linspace(0, 1, x.shape[0]) # Left to right -> more corruption\n",
    "print('Amount of corruption:', amount)\n",
    "noised_x = corrupt(x, amount, \"rand\")\n",
    "\n",
    "# Plotting the noised version\n",
    "axs[1].set_title('Corrupted data (-- amount increases -->)')\n",
    "axs[1].imshow(torchvision.utils.make_grid(noised_x)[0], cmap='Greys');\n",
    "\n",
    "# Plotting proportion of noise to an input sample\n",
    "sample_portion = 1-amount\n",
    "axs[2].plot(amount, '.-')\n",
    "axs[2].plot(sample_portion, '.-')\n",
    "axs[2].set_xlabel('Time step')\n",
    "axs[2].set_ylabel('Portion')\n",
    "axs[2].legend(['Noise added', 'Original sample']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Training the diffusion model\n",
    "\n",
    "Configuration for training:<br>\n",
    "- Epochs: 10\n",
    "- Batch size: 256\n",
    "- Optimizer: Adam\n",
    "- Learning rate: 0.001\n",
    "- Loss function: MSE Loss\n",
    "- Model: UNet mini or UNet attention\n",
    "\n",
    "Comparison between UNet mini and UNet attention:<br>\n",
    "- Computational time:\n",
    "    - UNet mini: ~ 50 seconds\n",
    "    - UNet attention:\n",
    "- Last MSE Loss:\n",
    "    - UNet mini: ~ 0.0174\n",
    "    - UNet attention: ~0.0099\n",
    "- Qualitative results:\n",
    "    - UNet mini: The generated numbers look sparse and faint, some generated output does not look like an actual number\n",
    "    - UNet attention: The generated numbers look clearer and bolder. The generated output looks more like an actual number, but some still can not be recognized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader (you can mess with batch size)\n",
    "batch_size = 256\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# How many runs through the data should we do?\n",
    "n_epochs = 10\n",
    "\n",
    "# Create the network\n",
    "model_name = \"unet_mini\" # unet_mini or \"unet_attn\"\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net = UNetWrapper(model_name)\n",
    "net.to(device)\n",
    "\n",
    "# Our loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# The optimizer\n",
    "opt = torch.optim.Adam(net.parameters(), lr=1e-3) \n",
    "\n",
    "# Keeping a record of the losses for later viewing\n",
    "losses = []\n",
    "\n",
    "tick = time.time()\n",
    "# The training loop\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "\n",
    "    for x, y in train_dataloader:\n",
    "\n",
    "        # Get some data and prepare the corrupted version\n",
    "        x = x.to(device) # Data on the GPU\n",
    "        noise_amount = torch.rand(x.shape[0]).to(device) # Pick random noise amounts from a unit uniform distribution\n",
    "        noisy_x = corrupt(x, noise_amount, rand_fn=\"rand\") # Create our noisy x\n",
    "\n",
    "        # Get the model prediction\n",
    "        pred = net(noisy_x)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = loss_fn(pred, x) # How close is the output to the true 'clean' x?\n",
    "\n",
    "        # Backprop and update the params:\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # Store the loss for later\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # Print our the average of the loss values for this epoch:\n",
    "    avg_loss = sum(losses[-len(train_dataloader):])/len(train_dataloader)\n",
    "    print(f'Finished epoch {epoch}. Average loss for this epoch: {avg_loss:05f}')\n",
    "\n",
    "# print the computational time\n",
    "tock = time.time()\n",
    "print(f\"Training took {tock - tick:.2f} seconds\")\n",
    "\n",
    "# View the loss curve\n",
    "plt.plot(losses)\n",
    "plt.ylim(0, 0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Sampling from the random inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown Sampling strategy: Break the process into 5 steps and move 1/5'th of the way there each time:\n",
    "n_steps = 10 # < - change number of steps here to get higher quality results (hopefully)\n",
    "x = torch.rand(8, 1, 28, 28).to(device) # Start from random\n",
    "step_history = [x.detach().cpu()]\n",
    "pred_output_history = []\n",
    "\n",
    "for i in range(n_steps):\n",
    "    \n",
    "    with torch.no_grad(): # No need to track gradients during inference\n",
    "        pred = net(x) # Predict the denoised x0\n",
    "    pred_output_history.append(pred.detach().cpu()) # Store model output for plotting\n",
    "    mix_factor = 1/(n_steps - i) # How much we move towards the prediction\n",
    "    x = x*(1-mix_factor) + pred*mix_factor # Move part of the way there\n",
    "    step_history.append(x.detach().cpu()) # Store step for plotting\n",
    "\n",
    "fig, axs = plt.subplots(n_steps, 2, figsize=(7, 7), sharex=True)\n",
    "axs[0,0].set_title('x (model input)')\n",
    "axs[0,1].set_title('model prediction')\n",
    "for i in range(n_steps):\n",
    "    axs[i, 0].imshow(torchvision.utils.make_grid(step_history[i])[0].clip(0, 1), cmap='Greys')\n",
    "    axs[i, 1].imshow(torchvision.utils.make_grid(pred_output_history[i])[0].clip(0, 1), cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot losses and some samples\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Losses\n",
    "axs[0].plot(losses)\n",
    "axs[0].set_ylim(0, 0.1)\n",
    "axs[0].set_title('Loss over time')\n",
    "# Samples\n",
    "n_steps = 40\n",
    "x = torch.rand(64, 1, 28, 28).to(device)\n",
    "for i in range(n_steps):\n",
    "  noise_amount = torch.ones((x.shape[0], )).to(device) * (1-(i/n_steps)) # Starting high going low\n",
    "  with torch.no_grad():\n",
    "    pred = net(x)\n",
    "  mix_factor = 1/(n_steps - i)\n",
    "  x = x*(1-mix_factor) + pred*mix_factor\n",
    "\n",
    "axs[1].imshow(torchvision.utils.make_grid(x.detach().cpu(), nrow=8)[0].clip(0, 1), cmap='Greys')\n",
    "axs[1].set_title('Generated Samples');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Class-conditioned Diffusion model\n",
    "\n",
    "Now, let's control the outputs of the generated images, by adding and training class embedding to the original model.\n",
    "\n",
    "- reference: https://colab.research.google.com/github/huggingface/diffusion-models-class/blob/main/unit2/02_class_conditioned_diffusion_model_example.ipynb#scrollTo=k_d6S0Fwlahy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Define class-conditioned diffusion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCUNet(nn.Module):\n",
    "    \"\"\" Class-conditioned Diffusion model \"\"\"\n",
    "\n",
    "    def __init__(self, model_name=\"unet_attn\", class_emb_size=4):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.class_emb_size = class_emb_size\n",
    "        self.class_emb = nn.Embedding(10, class_emb_size)\n",
    "\n",
    "        if model_name == \"unet_mini\":\n",
    "            self.model = BasicUNet(1 + class_emb_size, 1)\n",
    "        elif model_name == \"unet_attn\":\n",
    "            self.model = UNet2DModel(\n",
    "                sample_size=28,           # the target image resolution\n",
    "                in_channels=1 + class_emb_size, # Additional input channels for class cond.\n",
    "                out_channels=1,           # the number of output channels\n",
    "                layers_per_block=2,       # how many ResNet layers to use per UNet block\n",
    "                block_out_channels=(32, 64, 64), \n",
    "                down_block_types=( \n",
    "                    \"DownBlock2D\",        # a regular ResNet downsampling block\n",
    "                    \"AttnDownBlock2D\",    # a ResNet downsampling block with spatial self-attention\n",
    "                    \"AttnDownBlock2D\",\n",
    "                ), \n",
    "                up_block_types=(\n",
    "                    \"AttnUpBlock2D\", \n",
    "                    \"AttnUpBlock2D\",      # a ResNet upsampling block with spatial self-attention\n",
    "                    \"UpBlock2D\",          # a regular ResNet upsampling block\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    def forward(self, x, class_labels):\n",
    "        BS, C, W, H = x.shape\n",
    "\n",
    "        class_cond = self.class_emb(class_labels) # Map to embedding dimension\n",
    "        class_cond = class_cond.view(BS, class_cond.shape[1], 1, 1).expand(BS, class_cond.shape[1], W, H)\n",
    "        x = torch.cat((x, class_cond), dim=1) # (BS, C+4, W, H)\n",
    "\n",
    "        if self.model_name == \"unet_mini\":\n",
    "            return self.model(x)\n",
    "        elif self.model_name == \"unet_attn\":\n",
    "            return self.model(x, 0).sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Training the class-conditioned model\n",
    "\n",
    "Configuration for training:<br>\n",
    "- Epochs: 10\n",
    "- Batch size: 256\n",
    "- Optimizer: Adam\n",
    "- Learning rate: 0.001\n",
    "- Loss function: MSE Loss\n",
    "- Model: UNet mini or UNet attention\n",
    "\n",
    "Comparison between UNet mini and UNet attention:<br>\n",
    "- Computational time:\n",
    "    - UNet mini: ~ 50 seconds\n",
    "    - UNet attention: ~ 5 minutes\n",
    "- Last MSE Loss:\n",
    "    - The loss values of both models are generally lower than the previous section.\n",
    "    - UNet mini: 0.0146\n",
    "    - UNet attention: ~0.009\n",
    "- Qualitative results:\n",
    "    - UNet mini: The generated numbers look more like an actual number, when comparing to the previous section. And we are able to condition the output to a specific class.\n",
    "    - UNet attention: The generated numbers look clearer and bolder, and are able to condition the output to a specific class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader (you can mess with batch size)\n",
    "batch_size = 256\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# How many runs through the data should we do?\n",
    "n_epochs = 10\n",
    "\n",
    "# Create the network\n",
    "model_name = \"unet_mini\" # unet_mini or \"unet_attn\"\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net = CCUNet(model_name)\n",
    "net.to(device)\n",
    "\n",
    "# Our loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# The optimizer\n",
    "opt = torch.optim.Adam(net.parameters(), lr=1e-3) \n",
    "\n",
    "# Keeping a record of the losses for later viewing\n",
    "losses = []\n",
    "\n",
    "tick = time.time()\n",
    "# The training loop\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "\n",
    "    for x, y in train_dataloader:\n",
    "\n",
    "        # Get some data and prepare the corrupted version\n",
    "        x = x.to(device) # Data on the GPU\n",
    "        y = y.to(device) # Labels on the GPU\n",
    "        noise_amount = torch.rand(x.shape[0]).to(device) # Pick random noise amounts\n",
    "        noisy_x = corrupt(x, noise_amount, rand_fn=\"rand\") # Create our noisy x\n",
    "\n",
    "        # Get the model prediction\n",
    "        pred = net(noisy_x, y)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = loss_fn(pred, x) # How close is the output to the true 'clean' x?\n",
    "\n",
    "        # Backprop and update the params:\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # Store the loss for later\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # Print our the average of the loss values for this epoch:\n",
    "    avg_loss = sum(losses[-len(train_dataloader):])/len(train_dataloader)\n",
    "    print(f'Finished epoch {epoch}. Average loss for this epoch: {avg_loss:05f}')\n",
    "\n",
    "tock = time.time()\n",
    "print(f\"Training took {tock - tick:.2f} seconds\")\n",
    "\n",
    "# View the loss curve\n",
    "plt.plot(losses)\n",
    "plt.ylim(0, 0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Sampling by conditioning the model with classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Prepare random x to start from, plus some desired labels y\n",
    "x = torch.rand(80, 1, 28, 28).to(device)\n",
    "y = torch.tensor([[i]*8 for i in range(10)]).flatten().to(device)\n",
    "\n",
    "n_steps = 50\n",
    "\n",
    "# Sampling loop\n",
    "for t in tqdm(range(n_steps)):\n",
    "\n",
    "    # Get model pred\n",
    "    with torch.no_grad():\n",
    "        pred = net(x, y)  # Again, note that we pass in our labels y\n",
    "\n",
    "    # Update sample with step\n",
    "    mix_factor = 1/(n_steps - i)\n",
    "    x = x*(1-mix_factor) + pred*mix_factor      \n",
    "\n",
    "# Show the results\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
    "ax.imshow(torchvision.utils.make_grid(x.detach().cpu().clip(-1, 1), nrow=8)[0].clip(0, 1), cmap='Greys')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
