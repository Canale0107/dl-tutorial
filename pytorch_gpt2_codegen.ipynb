{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Generation using GPT2\n",
    "\n",
    "GPT2 is LLM that is originally from OpenAI, and it's licensed under MIT License.<br>\n",
    "https://huggingface.co/docs/transformers/en/model_doc/gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installation\n",
    "!pip install transformers[torch] datasets\n",
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Code dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset code_search_net (/net/papilio/storage7/phusaeng/.cache/huggingface/datasets/code_search_net/python/1.0.0/8f2524e6b62f65af5f5d65c53715c654db7b08dc93e0b7bcce2ab2f286a75be1)\n",
      "Found cached dataset code_search_net (/net/papilio/storage7/phusaeng/.cache/huggingface/datasets/code_search_net/python/1.0.0/8f2524e6b62f65af5f5d65c53715c654db7b08dc93e0b7bcce2ab2f286a75be1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training samples: 412178\n",
      "total validation samples: 23107\n",
      "we now re-sample the data to reduce the training time:\n",
      "Choosing 3.0% of the data for training.\n",
      "Choosing 30.0% of the data for validation.\n",
      "total training samples: 12365\n",
      "total validation samples: 6932\n"
     ]
    }
   ],
   "source": [
    "code_dataset_train = load_dataset(\"code_search_net\", \"python\", split=\"train\")\n",
    "code_dataset_validation = load_dataset(\"code_search_net\", \"python\", split=\"validation\")\n",
    "\n",
    "print(f\"total training samples: {code_dataset_train.num_rows}\")\n",
    "print(f\"total validation samples: {code_dataset_validation.num_rows}\")\n",
    "\n",
    "print(\"we now re-sample the data to reduce the training time:\")\n",
    "# sample portion of the data\n",
    "def trainDataPct(dataset, pct=1):\n",
    "    return dataset.select(range(int(len(dataset)*pct)))\n",
    "\n",
    "training_percentage = 0.075\n",
    "validation_percentage = 0.3\n",
    "print(f\"Choosing {training_percentage*100}% of the data for training.\")\n",
    "print(f\"Choosing {validation_percentage*100}% of the data for validation.\")\n",
    "\n",
    "code_dataset_train = trainDataPct(code_dataset_train, pct=training_percentage) # 7.5% of the data = 30913 training samples\n",
    "code_dataset_validation = trainDataPct(code_dataset_validation, pct=0.3)\n",
    "\n",
    "print(f\"total training samples: {code_dataset_train.num_rows}\")\n",
    "print(f\"total validation samples: {code_dataset_validation.num_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please write a function that Create and return an API context\n",
      "\n",
      "The function name is: AppBuilder.create_api_context\n",
      "\n",
      "def create_api_context(self, cls):\n",
      "        \"\"\"Create and return an API context\"\"\"\n",
      "        return self.api_context_schema().load({\n",
      "            \"name\": cls.name,\n",
      "            \"cls\": cls,\n",
      "            \"inst\": [],\n",
      "            \"conf\": self.conf.get_api_service(cls.name),\n",
      "            \"calls\": self.conf.get_api_calls(),\n",
      "            \"shared\": {},  # Used per-API to monitor state\n",
      "            \"log_level\": self.conf.get_log_level(),\n",
      "            \"callback\": self.receive\n",
      "            })\n"
     ]
    }
   ],
   "source": [
    "# visualize a sample\n",
    "df_train = code_dataset_train.to_pandas()\n",
    "def get_sample():\n",
    "    # randomly pick index for a row to be displayed\n",
    "    idx = torch.randint(0, len(df_train), (1,)).item()\n",
    "    # Get the text descriptions according to the selected idx\n",
    "    func_doc_str = df_train[\"func_documentation_string\"][idx]\n",
    "    func_name = df_train[\"func_name\"][idx]\n",
    "    whole_func_string = df_train[\"whole_func_string\"][idx]\n",
    "    # create query and answer prompts\n",
    "    query = \"\"\"Please write a function that {instruction}\"\"\"\n",
    "    fucntion_name = \"\"\"\\n\\nThe function name is: {func_name}\"\"\"\n",
    "    out = query.format(instruction=func_doc_str) + fucntion_name.format(func_name=func_name) + \"\\n\\n\" + whole_func_string\n",
    "    print(out)\n",
    "\n",
    "get_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /net/papilio/storage7/phusaeng/.cache/huggingface/datasets/code_search_net/python/1.0.0/8f2524e6b62f65af5f5d65c53715c654db7b08dc93e0b7bcce2ab2f286a75be1/cache-6af430db4a196fa8_*_of_00016.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /net/papilio/storage7/phusaeng/.cache/huggingface/datasets/code_search_net/python/1.0.0/8f2524e6b62f65af5f5d65c53715c654db7b08dc93e0b7bcce2ab2f286a75be1/cache-696e155c3838fd2f_*_of_00016.arrow\n"
     ]
    }
   ],
   "source": [
    "# preparing prompt and encode text to token ids with tokenizer\n",
    "def preprocess_function(examples):\n",
    "    \"\"\" prompt template for training the LM \"\"\"\n",
    "    # print(examples[\"func_documentation_string\"], examples[\"func_name\"], examples[\"whole_func_string\"])\n",
    "    query = \"\"\"Please write a function that {instruction}\"\"\"\n",
    "    fucntion_name = \"\"\"\\n\\nThe function name is: {func_name}\"\"\"\n",
    "    out_list = [\n",
    "        query.format(instruction=func_doc_str) + fucntion_name.format(func_name=func_name) + \"\\n\\n\" + whole_func_string\n",
    "        for func_doc_str, func_name, whole_func_string in \n",
    "        zip(examples[\"func_documentation_string\"], examples[\"func_name\"], examples[\"whole_func_string\"])\n",
    "    ]\n",
    "    out = tokenizer(out_list)\n",
    "    return out\n",
    "\n",
    "tokenized_train = code_dataset_train.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=16,\n",
    "    remove_columns=code_dataset_train.column_names\n",
    ")\n",
    "tokenized_validation = code_dataset_validation.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=16,\n",
    "    remove_columns=code_dataset_validation.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /net/papilio/storage7/phusaeng/.cache/huggingface/datasets/code_search_net/python/1.0.0/8f2524e6b62f65af5f5d65c53715c654db7b08dc93e0b7bcce2ab2f286a75be1/cache-4aecb19653e8f685_*_of_00016.arrow\n",
      "Loading cached processed dataset at /net/papilio/storage7/phusaeng/.cache/huggingface/datasets/code_search_net/python/1.0.0/8f2524e6b62f65af5f5d65c53715c654db7b08dc93e0b7bcce2ab2f286a75be1/cache-e8b4402d3571009b_*_of_00016.arrow\n"
     ]
    }
   ],
   "source": [
    "# after tokenized it, some samples may have the length that is longer than \n",
    "# the context size of the GPT2, which is 1024 in this case.\n",
    "# we need to group the text into smaller chunks with a specified block size that is less than 1024\n",
    "block_size = 700\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()} # sum of list is concatenation\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of block_size.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "tokenized_train = tokenized_train.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    num_proc=16,\n",
    ")\n",
    "tokenized_validation = tokenized_validation.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    num_proc=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /net/papilio/storage7/phusaeng/.cache/huggingface/datasets/code_search_net/python/1.0.0/8f2524e6b62f65af5f5d65c53715c654db7b08dc93e0b7bcce2ab2f286a75be1/cache-80196fec92e45b46.arrow\n",
      "/net/papilio/storage6/phusaeng/anaconda3/envs/pytorch2/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "run_name = \"gpt2-python-code-search-test\"\n",
    "out_dir = f\"out/{run_name}\"\n",
    "batch_size = 6 # increasing batch size can speed up training too, but may require more GPU memory\n",
    "epochs = 1\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=out_dir,\n",
    "    run_name=run_name,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50, # log the training loss for every 50 steps\n",
    "    eval_steps=100, # evaluate and show the validation result every 100 steps\n",
    "    save_steps=300,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_validation.shuffle(42).select(range(200)), # only use 200 validation samples during training, which will be much faster\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset code_search_net (/net/papilio/storage7/phusaeng/.cache/huggingface/datasets/code_search_net/python/1.0.0/8f2524e6b62f65af5f5d65c53715c654db7b08dc93e0b7bcce2ab2f286a75be1)\n"
     ]
    }
   ],
   "source": [
    "code_dataset_validation = load_dataset(\"code_search_net\", \"python\", split=\"validation\")\n",
    "df_valid = code_dataset_validation.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def get_prompt(examples, idx=None):\n",
    "    # if no idx, we randomly select one\n",
    "    if idx is None:\n",
    "        idx = torch.randint(0, len(examples), (1,)).item()\n",
    "    print(f\"idx: {idx}\")\n",
    "    query = \"\"\"Please write a function that {instruction}\"\"\"\n",
    "    fucntion_name = \"\"\"\\n\\nThe function name is: {func_name}\"\"\"\n",
    "    out_prompt = query.format(instruction=examples[\"func_documentation_string\"][idx]) + fucntion_name.format(func_name=examples[\"func_name\"][idx])\n",
    "    return out_prompt\n",
    "\n",
    "def generate(model, tokenizer, text, device=\"cuda\", max_new_tokens=400, include_input=True):\n",
    "    token_ids = tokenizer.encode(text, add_special_tokens=False, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            token_ids.to(device),\n",
    "            do_sample=True,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=1,\n",
    "            # repetition_penalty=1.1,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            bos_token_id=tokenizer.bos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    # output = tokenizer.decode(output_ids.tolist()[0][token_ids.size(1):])\n",
    "    if not include_input:\n",
    "        output = tokenizer.decode(output_ids.tolist()[0][token_ids.size(1):])\n",
    "    else:\n",
    "        output = tokenizer.decode(output_ids.tolist()[0])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model params: 124439808\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Not fine-tuned GPT (the default pretrained GPT2):\n",
    "model_name = \"gpt2\"\n",
    "org_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Fine-tuned GPT on the code dataset:\n",
    "model_name = \"out/gpt2-python-code-search-test/checkpoint-200\"\n",
    "trained_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "print(f\"model params: {count_parameters(trained_model)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference with pre-trained GPT2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: 29845\n",
      "prompt: Please write a function that Return the version of by with regex intead of importing it\n",
      "\n",
      "The function name is: get_version\n"
     ]
    }
   ],
   "source": [
    "text = get_prompt(df_train, idx=None)\n",
    "# text = \"please write the function that find cosine similarity between two vectors\"\n",
    "max_new_tokens = 512\n",
    "print(f\"prompt: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please write a function that Return the version of by with regex intead of importing it\n",
      "\n",
      "The function name is: get_version_of_by\n",
      "\n",
      "It returns the version, if it has a version. If the version is not empty you can pass either 0 or nil.\n",
      "\n",
      "If version is empty do:\n",
      "\n",
      "get_version :(version, _) => nil | json:value @json\n",
      "\n",
      "It returns the version number with the hash of the version used.\n",
      "\n",
      "The function accepts the following arguments:\n",
      "\n",
      "version: number of the version of JSON or JSON.string\n",
      "\n",
      "json: string to use in converting the json type to json:string. If no arguments are provided it will just pass 0 and get_version_ref is passed as the string.\n",
      "\n",
      "The json value will be stored on disk in memory. The json version will be the current one.\n",
      "\n",
      "Return the number of versions of a given version string (or a reference to the original JSON) to be converted to JSON with get_version_ref if None is given.\n",
      "\n",
      "The function will not return 0 or None if the json is not available for this format.\n",
      "\n",
      "If the json value is greater than or equal to 0 or any other type that is available for this format in a format as format and json.string does not match:\n",
      "\n",
      "json: json_string => 0\n",
      "\n",
      "If the json value is not greater than or equal to NaN, json: n == N, json: n <= 1, json: n <= N.\n",
      "\n",
      "The function will NOT return a None if this format.\n",
      "\n",
      "If you want to convert one of jsons to a type and return None the value will be passed as the result of a check.\n",
      "\n",
      "To convert JSON to N or NaN, json can be used for the conversion of strings, but this is required because N does not meet the standard N-type conversion. You can do other conversions by using json:value -like json:type.\n",
      "\n",
      "If json is not available and you want the json type to match for the JSON with which you want a reference - N, but json:N=N will throw an exception if NaN is not provided.\n",
      "\n",
      "You can provide JSON :value or json:type with json:type, but json does not have an argument type, json:type will be passed as a default value. If this is not used if json in the default format doesn't accept N, use json:value. You can write a method that returns any other type that would return a value like json:name\n",
      "\n",
      "JSON\n"
     ]
    }
   ],
   "source": [
    "print(generate(\n",
    "    model=org_model, \n",
    "    tokenizer=tokenizer, \n",
    "    text=text, \n",
    "    device=device,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    include_input=True\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference with fine-tuned GPT2 model on the code dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please write a function that Return the version of by with regex intead of importing it\n",
      "\n",
      "The function name is: get_version\n",
      "\n",
      "def get_version(self):\n",
      " from sys import sys import regex_intead\n",
      " version = regex.decode(self.version, re.SEMOTIC('-1-',(2,3,4) for i in re.samples.layers())):\n",
      " if not version[0] except NotImplementedError:\n",
      " version.update(self.version)\n",
      "version = regex.decode(self.version, re.SEMOTIC('-1-',(2,3,4) for i in re.samples.layers())):\n",
      " version = regex.decode(self.version, re.SEMOTIC('-1-',(2,3,4) for i in re.samples.layers())):\n",
      " version = regex.decode(self.version, re.SEMOTIC('-1-',(2,3,4) for i in re.samples.layers())):\n",
      " version = regex.decode(self.version, re.SEMOTIC('-1-',(2,3,4) for i in re.samples.layers())):\n",
      " version = regex.decode(self.version, re.SEMOTIC('-1-',(2,3,4) for i in re.samples.layers())):\n",
      " version = regex.decode(self.version, re.SEMOTIC('-1-',(2,3,4) for i in re.samples.layers())):\n",
      " version = regex.decode(self.version, re.SEMOTIC('-1-',(2,3,4) for i in re.samples.layers())):\n",
      " version = regex.decode(self.version, re.SEMOTIC('-1-',(2,3,4) for i in re.samples.layers())):\n",
      " version = regex.decode(self.version, re.SEMOTIC('-1-',(2,3,4) for i in re.samples.layers())):\n",
      " version = regex.decode(self.version, re.SEMOTIC('-1-',(2,3,4) for i in re.samples.layers())):\n",
      " version = regex.decode(\n"
     ]
    }
   ],
   "source": [
    "print(generate(\n",
    "    model=trained_model, \n",
    "    tokenizer=tokenizer, \n",
    "    text=text, \n",
    "    device=device,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    include_input=True\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the performance of the two models on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# evaluate function\n",
    "def get_prompt_answer(examples, idx=None):\n",
    "    # if no idx, we randomly select one\n",
    "    if idx is None:\n",
    "        idx = torch.randint(0, len(examples), (1,)).item()\n",
    "    # print(f\"idx: {idx}\")\n",
    "    query = \"\"\"Please write a function that {instruction}\"\"\"\n",
    "    fucntion_name = \"\"\"\\n\\nThe function name is: {func_name}\"\"\"\n",
    "    prompt = query.format(instruction=examples[\"func_documentation_string\"][idx]) + fucntion_name.format(func_name=examples[\"func_name\"][idx])\n",
    "    answer = examples[\"whole_func_string\"][idx]\n",
    "    prompt += \"\\n\\n\" + answer\n",
    "    return prompt\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\" prompt template for training the LM \"\"\"\n",
    "    query = \"\"\"Please write a function that {instruction}\"\"\"\n",
    "    fucntion_name = \"\"\"\\n\\nThe function name is: {func_name}\"\"\"\n",
    "    out = tokenizer([\n",
    "        query.format(instruction=func_doc_str) + fucntion_name.format(func_name=func_name) + \"\\n\\n\" + whole_func_string\n",
    "        for func_doc_str, func_name, whole_func_string in \n",
    "        zip(examples[\"func_documentation_string\"], examples[\"func_name\"], examples[\"whole_func_string\"])\n",
    "    ])\n",
    "    return out\n",
    "\n",
    "def evaluate(model, encodings, stride=512):\n",
    "    \"\"\" https://huggingface.co/docs/transformers/en/perplexity \"\"\"\n",
    "    seq_len = encodings.input_ids.size(1)\n",
    "    max_length = model.config.n_positions\n",
    "    print(f\"seq_len: {seq_len}, max_length: {max_length}\")\n",
    "    nlls = []\n",
    "    prev_end_loc = 0\n",
    "    for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "        end_loc = min(begin_loc + max_length, seq_len)\n",
    "        trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "        target_ids = input_ids.clone()\n",
    "        # set to -100 to ignore the loss over the context input\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "        target_ids[:, :-trg_len] = -100 \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=target_ids)\n",
    "            # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "            # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
    "            # to the left by 1.\n",
    "            neg_log_likelihood = outputs.loss\n",
    "        nlls.append(neg_log_likelihood)\n",
    "        prev_end_loc = end_loc\n",
    "        if end_loc == seq_len:\n",
    "            break\n",
    "    cse = torch.stack(nlls).mean()\n",
    "    return cse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (79384 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_len: 79384, max_length: 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/156 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 154/156 [00:11<00:00, 13.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy loss: 1.9215\n",
      "Perplexity: 6.8309\n",
      "seq_len: 79384, max_length: 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 154/156 [00:11<00:00, 13.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy loss: 1.5223\n",
      "Perplexity: 4.5826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>CSE</th>\n",
       "      <th>PPL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT2</td>\n",
       "      <td>1.921461</td>\n",
       "      <td>6.830929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GPT2 /w ft</td>\n",
       "      <td>1.522273</td>\n",
       "      <td>4.582630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model_name       CSE       PPL\n",
       "0        GPT2  1.921461  6.830929\n",
       "1  GPT2 /w ft  1.522273  4.582630"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# get the validation dataset \n",
    "num_test_samples = 100\n",
    "text = [get_prompt_answer(df_valid, idx=idx) for idx in range(num_test_samples)]\n",
    "encodings = tokenizer(\"\\n\\n\".join(text), return_tensors=\"pt\")\n",
    "models = {\n",
    "    \"GPT2\": org_model,\n",
    "    \"GPT2 /w ft\": trained_model\n",
    "}\n",
    "df_log = pd.DataFrame()\n",
    "stride_len = 512\n",
    "for model_name, model in models.items():\n",
    "    mean_cse = evaluate(model, encodings, stride_len)\n",
    "    ppl = torch.exp(mean_cse)\n",
    "    print(f\"Cross-entropy loss: {mean_cse:.4f}\")\n",
    "    print(f\"Perplexity: {ppl:.4f}\")\n",
    "    log = {\n",
    "        \"model_name\": model_name,\n",
    "        \"CSE\": mean_cse.item(),\n",
    "        \"PPL\": ppl.item()\n",
    "    }\n",
    "    df_log = pd.concat([df_log, pd.DataFrame([log])], ignore_index=True)\n",
    "df_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
