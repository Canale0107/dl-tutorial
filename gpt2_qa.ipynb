{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2を用いた質疑応答システム\n",
    "\n",
    "## はじめに\n",
    "\n",
    "このノートブックでは、GPT2と呼ばれる大規模言語モデル(LLM)を用いた質疑応答システムを体験できます。<br>\n",
    "GPT2とは、もともとOpenAI社が開発したLLMであり、今はMITライセンスが付与され全世界に無料公開されています。<br>\n",
    "昨今有名なChatGPTは、このGPT2をさらに改良したGPT3.5およびGPT4(Plus版)が使われています。\n",
    "\n",
    "## 使用するモデル\n",
    "\n",
    "ここでは、OpenAI社または[rinna株式会社](https://corp.rinna.co.jp/)によって学習された、英語または日本語のGPT2モデルを使用します。<br>\n",
    "これらのモデルは、[MIT license](https://opensource.org/license/mit/)で提供されています。<br>\n",
    "下記が、今回使用するモデルのドキュメントとなっております。\n",
    "\n",
    "https://huggingface.co/openai-community/gpt2<br>\n",
    "https://huggingface.co/rinna/japanese-gpt2-medium\n",
    "\n",
    "## 1. ライブラリをインストールする\n",
    "\n",
    "ここでは、必要なライブラリをインストールします。<br>\n",
    "もしうまく動かない場合は、カーネルをリスタートするとよいみたいです。\n",
    "\n",
    "参考: https://www.cloudnotes.tech/entry/rinna_gpt_jp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers\n",
    "! pip install sentencepiece\n",
    "! pip install accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ライブラリをインポートする\n",
    "\n",
    "ここでは、必要なライブラリをインポートします。<br>\n",
    "GPUを利用する設定にしていることを確認してから実行してください。<br>\n",
    "うまく設定できている場合は、以下のセルを実行すると `cuda` と出力されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 言語設定\n",
    "\n",
    "このプログラムを動かすうえで、使用する言語はここで変更できます。<br>\n",
    "`use_japanese` の値を変更してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 日本語のGPTモデルを使用する際はTrue, 英語のGPTモデルを使用する際はFalseにしてください。\n",
    "# If you use Japanese GPT model, set True, otherwise if you use English GPT model, set False.\n",
    "use_japanese = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. モデルとトークナイザを定義する\n",
    "\n",
    "モデルとトークナイザを定義します。<br>\n",
    "今回は時間（と計算コスト）の都合上、すでに訓練されたモデルを使用します。<br>\n",
    "ファインチューニングという手法を利用することで、すでに訓練されたモデルの精度をさらに向上させることができます。<br>\n",
    "ただし、モデルのロードに少々時間がかかります。お待ちください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_japanese:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"rinna/japanese-gpt2-medium\", use_fast=False)\n",
    "    tokenizer.do_lower_case = True  # due to some bug of tokenizer config loading\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"rinna/japanese-gpt2-medium\").to(device)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 質問を定義する\n",
    "\n",
    "次の `question` 変数にお好きな質問を設定して、実行してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"東京工業大学のいいところは何ですか。\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 回答を得る\n",
    "\n",
    "上の質問を定義して、以下のセルを実行すると、回答が得られます。<br>\n",
    "回答の生成に少々時間がかかりますのでお待ちください。\n",
    "\n",
    "この5番と6番は何回でも繰り返すことができます。<br>\n",
    "また、6番のみを繰り返すと、同じ質問でも違う回答を得ることができます。\n",
    "\n",
    "もし言語設定を変更したい場合は、3番を変更して、その後3番から順番に実行してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"{} \".format(question)\n",
    "token_ids = tokenizer.encode(text, add_special_tokens=False, return_tensors='pt')\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(device),\n",
    "        do_sample=True,\n",
    "        max_new_tokens=128,\n",
    "        temperature=0.7,\n",
    "        repetition_penalty=1.1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0][token_ids.size(1):])\n",
    "print(\"-\"*10)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idsat_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
